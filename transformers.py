import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, average_precision_score
from sklearn.preprocessing import LabelEncoder
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from transformers import AutoTokenizer, AutoModel
from torch.optim import AdamW
import warnings
warnings.filterwarnings('ignore')

# –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ GPU
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"–ò—Å–ø–æ–ª—å–∑—É–µ–º–æ–µ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {device}")

# –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
df = pd.read_csv('train_m.csv')
text_col = df.columns[1]
label_col = df.columns[2]

print("=== –ê–ù–ê–õ–ò–ó –î–ê–¢–ê–°–ï–¢–ê ===")
print(f"–†–∞–∑–º–µ—Ä –¥–∞—Ç–∞—Å–µ—Ç–∞: {len(df)}")
print(f"–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤:\n{df[label_col].value_counts().sort_index()}")

# –£–¥–∞–ª—è–µ–º –∫–ª–∞—Å—Å—ã —Å –º–∞–ª—ã–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º –ø—Ä–∏–º–µ—Ä–æ–≤
min_samples = 2
class_counts = df[label_col].value_counts()
valid_classes = class_counts[class_counts >= min_samples].index
df = df[df[label_col].isin(valid_classes)]

print(f"–ü–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏: {len(df)} –ø—Ä–∏–º–µ—Ä–æ–≤")

# –ö–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –º–µ—Ç–æ–∫
le = LabelEncoder()
df['label_encoded'] = le.fit_transform(df[label_col])

# –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö
train_df, test_df = train_test_split(df, test_size=0.2, random_state=42, stratify=df['label_encoded'])
train_df, val_df = train_test_split(train_df, test_size=0.1, random_state=42, stratify=train_df['label_encoded'])

print(f"\n–†–∞–∑–¥–µ–ª–µ–Ω–∏–µ: Train {len(train_df)}, Val {len(val_df)}, Test {len(test_df)}")

# –ú–æ–¥–µ–ª—å —Å –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ–º pooler_output
class ToxicClassifier(nn.Module):
    def __init__(self, model_name, num_classes):
        super(ToxicClassifier, self).__init__()
        self.bert = AutoModel.from_pretrained(model_name)
        self.dropout = nn.Dropout(0.1)
        self.classifier = nn.Linear(self.bert.config.hidden_size, num_classes)
        
    def forward(self, input_ids, attention_mask):
        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)
        
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø–µ—Ä–≤—ã–π —Ç–æ–∫–µ–Ω [CLS] –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
        pooled_output = outputs.last_hidden_state[:, 0, :]
        output = self.dropout(pooled_output)
        return self.classifier(output)

# –î–∞—Ç–∞—Å–µ—Ç
class CommentDataset(Dataset):
    def __init__(self, texts, labels, tokenizer, max_length=128):
        self.texts = texts
        self.labels = labels
        self.tokenizer = tokenizer
        self.max_length = max_length
        
    def __len__(self):
        return len(self.texts)
    
    def __getitem__(self, idx):
        text = str(self.texts.iloc[idx])
        label = self.labels.iloc[idx]
        
        encoding = self.tokenizer(
            text,
            truncation=True,
            padding='max_length',
            max_length=self.max_length,
            return_tensors='pt'
        )
        
        return {
            'input_ids': encoding['input_ids'].flatten(),
            'attention_mask': encoding['attention_mask'].flatten(),
            'labels': torch.tensor(label, dtype=torch.long)
        }

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è
model_name = "distilbert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(model_name)
num_classes = len(le.classes_)

model = ToxicClassifier(model_name, num_classes).to(device)

# –î–∞—Ç–∞—Å–µ—Ç—ã –∏ –¥–∞—Ç–∞–ª–æ–∞–¥–µ—Ä—ã
train_dataset = CommentDataset(train_df[text_col], train_df['label_encoded'], tokenizer)
val_dataset = CommentDataset(val_df[text_col], val_df['label_encoded'], tokenizer)
test_dataset = CommentDataset(test_df[text_col], test_df['label_encoded'], tokenizer)

train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=8)
test_loader = DataLoader(test_dataset, batch_size=8)

# –í–µ—Å–∞ –∫–ª–∞—Å—Å–æ–≤
class_counts = np.bincount(df['label_encoded'])
class_weights = torch.tensor(
    [1.0 / count if count > 0 else 1.0 for count in class_counts],
    dtype=torch.float32
).to(device)
class_weights = class_weights / class_weights.sum() * num_classes

print(f"\n–í–µ—Å–∞ –∫–ª–∞—Å—Å–æ–≤: {class_weights.cpu().numpy()}")

# –û–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä –∏ —Ñ—É–Ω–∫—Ü–∏—è –ø–æ—Ç–µ—Ä—å
optimizer = AdamW(model.parameters(), lr=2e-5)
criterion = nn.CrossEntropyLoss(weight=class_weights)

# –û–±—É—á–µ–Ω–∏–µ
def train_epoch(model, dataloader, optimizer, criterion):
    model.train()
    total_loss = 0
    correct_predictions = 0
    
    for batch in dataloader:
        input_ids = batch['input_ids'].to(device)
        attention_mask = batch['attention_mask'].to(device)
        labels = batch['labels'].to(device)
        
        optimizer.zero_grad()
        outputs = model(input_ids, attention_mask)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()
        
        total_loss += loss.item()
        _, preds = torch.max(outputs, dim=1)
        correct_predictions += torch.sum(preds == labels)
    
    accuracy = correct_predictions.double() / len(dataloader.dataset)
    avg_loss = total_loss / len(dataloader)
    
    return avg_loss, accuracy

# –í–∞–ª–∏–¥–∞—Ü–∏—è
def eval_model(model, dataloader, criterion):
    model.eval()
    total_loss = 0
    correct_predictions = 0
    all_preds = []
    all_labels = []
    all_probs = []
    
    with torch.no_grad():
        for batch in dataloader:
            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)
            labels = batch['labels'].to(device)
            
            outputs = model(input_ids, attention_mask)
            loss = criterion(outputs, labels)
            probs = torch.softmax(outputs, dim=1)
            
            total_loss += loss.item()
            _, preds = torch.max(outputs, dim=1)
            correct_predictions += torch.sum(preds == labels)
            
            all_preds.extend(preds.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())
            all_probs.extend(probs.cpu().numpy())
    
    accuracy = correct_predictions.double() / len(dataloader.dataset)
    avg_loss = total_loss / len(dataloader)
    
    return avg_loss, accuracy, all_preds, all_labels, all_probs

print("\n=== –ù–ê–ß–ê–õ–û –û–ë–£–ß–ï–ù–ò–Ø ===")
best_val_loss = float('inf')
patience = 3
patience_counter = 0

for epoch in range(5):
    print(f"\n–≠–ø–æ—Ö–∞ {epoch + 1}/5")
    
    # –û–±—É—á–µ–Ω–∏–µ
    train_loss, train_acc = train_epoch(model, train_loader, optimizer, criterion)
    print(f"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}")
    
    # –í–∞–ª–∏–¥–∞—Ü–∏—è
    val_loss, val_acc, val_preds, val_labels, val_probs = eval_model(model, val_loader, criterion)
    print(f"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}")
    
    # –†–∞–Ω–Ω—è—è –æ—Å—Ç–∞–Ω–æ–≤–∫–∞
    if val_loss < best_val_loss:
        best_val_loss = val_loss
        patience_counter = 0
        torch.save(model.state_dict(), 'best_model.pt')
        print("‚úÖ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –ª—É—á—à–∞—è –º–æ–¥–µ–ª—å")
    else:
        patience_counter += 1
        if patience_counter >= patience:
            print("üõë –†–∞–Ω–Ω—è—è –æ—Å—Ç–∞–Ω–æ–≤–∫–∞")
            break

# –ó–∞–≥—Ä—É–∑–∫–∞ –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏
model.load_state_dict(torch.load('best_model.pt'))

# –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ
print("\n=== –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï ===")
test_loss, test_acc, test_preds, test_labels, test_probs = eval_model(model, test_loader, criterion)
test_probs = np.array(test_probs)

# –ú–ï–¢–†–ò–ö–ò –ö–ê–ß–ï–°–¢–í–ê
print("\n" + "="*50)
print("–ú–ï–¢–†–ò–ö–ò –ö–ê–ß–ï–°–¢–í–ê –ù–ê –¢–ï–°–¢–ï")
print("="*50)

from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score

accuracy = accuracy_score(test_labels, test_preds)
macro_f1 = f1_score(test_labels, test_preds, average='macro')
weighted_f1 = f1_score(test_labels, test_preds, average='weighted')
precision = precision_score(test_labels, test_preds, average='macro')
recall = recall_score(test_labels, test_preds, average='macro')

print(f"\nüéØ –û–°–ù–û–í–ù–´–ï –ú–ï–¢–†–ò–ö–ò:")
print(f"Accuracy: {accuracy:.4f}")
print(f"Macro F1: {macro_f1:.4f}")
print(f"Weighted F1: {weighted_f1:.4f}")
print(f"Macro Precision: {precision:.4f}")
print(f"Macro Recall: {recall:.4f}")

# Classification Report
print(f"\nüìä CLASSIFICATION REPORT:")
print(classification_report(test_labels, test_preds, target_names=[f'Class {i}' for i in le.classes_]))

# Confusion Matrix
plt.figure(figsize=(8, 6))
cm = confusion_matrix(test_labels, test_preds)
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=[f'Class {i}' for i in le.classes_],
            yticklabels=[f'Class {i}' for i in le.classes_])
plt.title('Confusion Matrix - Test Set')
plt.ylabel('True Label')
plt.xlabel('Predicted Label')
plt.tight_layout()
plt.show()

# ROC-AUC –∏ PR-AUC
try:
    if num_classes == 2:
        roc_auc = roc_auc_score(test_labels, test_probs[:, 1])
        pr_auc = average_precision_score(test_labels, test_probs[:, 1])
        print(f"ROC-AUC: {roc_auc:.4f}")
        print(f"PR-AUC: {pr_auc:.4f}")
    else:
        roc_auc = roc_auc_score(test_labels, test_probs, multi_class='ovr', average='macro')
        pr_auc = average_precision_score(test_labels, test_probs, average='macro')
        print(f"Macro ROC-AUC: {roc_auc:.4f}")
        print(f"Macro PR-AUC: {pr_auc:.4f}")
except Exception as e:
    print(f"AUC –º–µ—Ç—Ä–∏–∫–∏: –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–æ ({e})")

# –ê–ù–ê–õ–ò–ó –†–ï–ó–£–õ–¨–¢–ê–¢–û–í
print("\n" + "="*50)
print("–ê–ù–ê–õ–ò–ó –†–ï–ó–£–õ–¨–¢–ê–¢–û–í")
print("="*50)

results_df = test_df.copy()
results_df['predicted'] = test_preds
results_df['correct'] = (results_df['label_encoded'] == results_df['predicted'])
results_df['confidence'] = np.max(test_probs, axis=1)
results_df['text_length'] = results_df[text_col].apply(len)

# –ê–Ω–∞–ª–∏–∑ –ø–æ –∫–ª–∞—Å—Å–∞–º
print("\nüìà –¢–û–ß–ù–û–°–¢–¨ –ü–û –ö–õ–ê–°–°–ê–ú:")
for class_id in le.classes_:
    class_data = results_df[results_df[label_col] == class_id]
    if len(class_data) > 0:
        accuracy_class = class_data['correct'].mean()
        print(f"–ö–ª–∞—Å—Å {class_id}: {accuracy_class:.1%} ({class_data['correct'].sum()}/{len(class_data)})")

# –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
fig, axes = plt.subplots(2, 2, figsize=(15, 10))

# 1. Accuracy –ø–æ –∫–ª–∞—Å—Å–∞–º
class_accuracies = []
for class_id in le.classes_:
    class_data = results_df[results_df[label_col] == class_id]
    accuracy = class_data['correct'].mean() if len(class_data) > 0 else 0
    class_accuracies.append(accuracy)

colors = ['green', 'orange', 'red', 'blue', 'purple']
axes[0, 0].bar(range(len(le.classes_)), class_accuracies, 
                color=colors[:len(le.classes_)], alpha=0.7)
axes[0, 0].set_xlabel('–ö–ª–∞—Å—Å')
axes[0, 0].set_ylabel('Accuracy')
axes[0, 0].set_title('Accuracy –ø–æ –∫–ª–∞—Å—Å–∞–º')
axes[0, 0].set_xticks(range(len(le.classes_)))
axes[0, 0].set_xticklabels([f'Class {i}' for i in le.classes_])
axes[0, 0].set_ylim(0, 1)

# 2. –£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏
correct_conf = results_df[results_df['correct']]['confidence'].mean()
incorrect_conf = results_df[~results_df['correct']]['confidence'].mean()

axes[0, 1].bar(['–ü—Ä–∞–≤–∏–ª—å–Ω—ã–µ', '–û—à–∏–±–∫–∏'], [correct_conf, incorrect_conf], 
                color=['green', 'red'], alpha=0.7)
axes[0, 1].set_ylabel('–°—Ä–µ–¥–Ω—è—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å')
axes[0, 1].set_title('–£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏')
axes[0, 1].set_ylim(0, 1)

# 3. –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –¥–ª–∏–Ω—ã —Ç–µ–∫—Å—Ç–∞
axes[1, 0].hist([results_df[results_df['correct']]['text_length'], 
                 results_df[~results_df['correct']]['text_length']],
                bins=20, alpha=0.7, label=['–ü—Ä–∞–≤–∏–ª—å–Ω—ã–µ', '–û—à–∏–±–∫–∏'], 
                color=['green', 'red'])
axes[1, 0].set_xlabel('–î–ª–∏–Ω–∞ —Ç–µ–∫—Å—Ç–∞')
axes[1, 0].set_ylabel('–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ')
axes[1, 0].set_title('–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –¥–ª–∏–Ω—ã —Ç–µ–∫—Å—Ç–∞')
axes[1, 0].legend()

# 4. –ú–∞—Ç—Ä–∏—Ü–∞ –æ—à–∏–±–æ–∫ (–ø—Ä–æ—Ü–µ–Ω—Ç—ã)
cm_percent = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
sns.heatmap(cm_percent, annot=True, fmt='.2%', cmap='Blues', ax=axes[1, 1],
            xticklabels=[f'Class {i}' for i in le.classes_],
            yticklabels=[f'Class {i}' for i in le.classes_])
axes[1, 1].set_title('Confusion Matrix (%)')
axes[1, 1].set_ylabel('True Label')
axes[1, 1].set_xlabel('Predicted Label')

plt.tight_layout()
plt.show()

# –î–ï–¢–ê–õ–¨–ù–´–ô –ê–ù–ê–õ–ò–ó –û–®–ò–ë–û–ö
print("\n" + "="*50)
print("–î–ï–¢–ê–õ–¨–ù–´–ô –ê–ù–ê–õ–ò–ó –û–®–ò–ë–û–ö")
print("="*50)

# –ê–Ω–∞–ª–∏–∑ –Ω–∞–∏–±–æ–ª–µ–µ –ø—Ä–æ–±–ª–µ–º–Ω—ã—Ö –ø–∞—Ä –∫–ª–∞—Å—Å–æ–≤
error_pairs = []
for true_class in range(len(le.classes_)):
    for pred_class in range(len(le.classes_)):
        if true_class != pred_class:
            count = cm[true_class, pred_class]
            if count > 0:
                error_pairs.append((true_class, pred_class, count))

error_pairs.sort(key=lambda x: x[2], reverse=True)

print("\nüß© –ù–ê–ò–ë–û–õ–ï–ï –ß–ê–°–¢–´–ï –û–®–ò–ë–ö–ò:")
for true_class, pred_class, count in error_pairs[:5]:
    print(f"True: Class {true_class} ‚Üí Pred: Class {pred_class}: {count} —Å–ª—É—á–∞–µ–≤")

# –ü—Ä–∏–º–µ—Ä—ã –æ—à–∏–±–æ–∫
print(f"\nüìù –ü–†–ò–ú–ï–†–´ –û–®–ò–ë–û–ß–ù–´–• –ü–†–ï–î–°–ö–ê–ó–ê–ù–ò–ô:")
errors_df = results_df[~results_df['correct']].head(3)
for idx, row in errors_df.iterrows():
    print(f"\n–¢–µ–∫—Å—Ç: {row[text_col][:150]}...")
    print(f"–ò—Å—Ç–∏–Ω–Ω—ã–π –∫–ª–∞—Å—Å: {row[label_col]}, –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–π: {row['predicted']}")
    print(f"–£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {row['confidence']:.3f}")

# –í–´–í–û–î–´ –ò –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò
print("\n" + "="*50)
print("–í–´–í–û–î–´ –ò –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò")
print("="*50)

# –û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞
if macro_f1 >= 0.75:
    rating = "‚úÖ –û–¢–õ–ò–ß–ù–û"
    recommendation = "–ú–æ–¥–µ–ª—å –≥–æ—Ç–æ–≤–∞ –∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é"
elif macro_f1 >= 0.65:
    rating = "‚ö†Ô∏è  –•–û–†–û–®–û" 
    recommendation = "–ú–æ–¥–µ–ª—å –º–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Å –Ω–µ–∫–æ—Ç–æ—Ä—ã–º–∏ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è–º–∏"
elif macro_f1 >= 0.55:
    rating = "‚ö†Ô∏è  –£–î–û–í–õ–ï–¢–í–û–†–ò–¢–ï–õ–¨–ù–û"
    recommendation = "–¢—Ä–µ–±—É–µ—Ç –¥–æ—Ä–∞–±–æ—Ç–∫–∏ –ø–µ—Ä–µ–¥ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º"
else:
    rating = "‚ùå  –ü–õ–û–•–û"
    recommendation = "–ù–µ –ø—Ä–∏–≥–æ–¥–Ω–∞ –¥–ª—è –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–æ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è"

print(f"–û–±—â–∞—è –æ—Ü–µ–Ω–∫–∞: {rating}")
print(f"–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è: {recommendation}")
print(f"–ö–ª—é—á–µ–≤–∞—è –º–µ—Ç—Ä–∏–∫–∞ (Macro F1): {macro_f1:.4f}")

# –ê–Ω–∞–ª–∏–∑ –ø—Ä–∏—á–∏–Ω
print(f"\nüîç –ö–õ–Æ–ß–ï–í–´–ï –ù–ê–ë–õ–Æ–î–ï–ù–ò–Ø:")

if weighted_f1 - macro_f1 > 0.1:
    print("‚Ä¢ –ó–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã–π –¥–∏—Å–±–∞–ª–∞–Ω—Å –∫–ª–∞—Å—Å–æ–≤ –≤–ª–∏—è–µ—Ç –Ω–∞ –∫–∞—á–µ—Å—Ç–≤–æ")
    
if incorrect_conf > 0.7:
    print("‚Ä¢ –ú–æ–¥–µ–ª—å —Å–ª–∏—à–∫–æ–º —É–≤–µ—Ä–µ–Ω–∞ –≤ –æ—à–∏–±–∫–∞—Ö - –≤–æ–∑–º–æ–∂–µ–Ω overfitting")
    
if len(results_df[results_df['text_length'] < 10]) > len(results_df) * 0.3:
    print("‚Ä¢ –ú–Ω–æ–≥–æ –∫–æ—Ä–æ—Ç–∫–∏—Ö —Ç–µ–∫—Å—Ç–æ–≤ - —Å–ª–æ–∂–Ω–æ –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏")

# –ö–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
print(f"\nüí° –ö–û–ù–ö–†–ï–¢–ù–´–ï –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò:")
if macro_f1 < 0.7:
    print("1. –£–≤–µ–ª–∏—á–∏—Ç—å –æ–±—ä–µ–º –æ–±—É—á–∞—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö")
    print("2. –ü–æ–ø—Ä–æ–±–æ–≤–∞—Ç—å –¥—Ä—É–≥–∏–µ –º–æ–¥–µ–ª–∏ (RoBERTa, DeBERTa)")
    print("3. –î–æ–±–∞–≤–∏—Ç—å —Ç–µ–∫—Å—Ç–æ–≤—É—é –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫—É")
    print("4. –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏—é –¥–∞–Ω–Ω—ã—Ö")
    
if len(error_pairs) > 0 and error_pairs[0][2] > len(test_df) * 0.1:
    print(f"5. –û–±—Ä–∞—Ç–∏—Ç—å –≤–Ω–∏–º–∞–Ω–∏–µ –Ω–∞ confusion –º–µ–∂–¥—É –∫–ª–∞—Å—Å–∞–º–∏ {error_pairs[0][0]} –∏ {error_pairs[0][1]}")
    
print(f"\nüéØ –§–ò–ù–ê–õ–¨–ù–´–ï –ú–ï–¢–†–ò–ö–ò:")
print(f"‚Ä¢ Macro F1: {macro_f1:.4f}")
print(f"‚Ä¢ Accuracy: {accuracy:.4f}")
print(f"‚Ä¢ Weighted F1: {weighted_f1:.4f}")
if 'roc_auc' in locals():
    print(f"‚Ä¢ ROC-AUC: {roc_auc:.4f}")
if 'pr_auc' in locals():
    print(f"‚Ä¢ PR-AUC: {pr_auc:.4f}")
