import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import MultinomialNB
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, confusion_matrix, f1_score, accuracy_score, roc_auc_score, average_precision_score
from sklearn.utils.class_weight import compute_class_weight
import nltk
import re
from collections import Counter
import warnings
warnings.filterwarnings('ignore')

# –°–∫–∞—á–∏–≤–∞–µ–º —Å—Ç–æ–ø-—Å–ª–æ–≤–∞ –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
try:
    nltk.data.find('corpora/stopwords')
except LookupError:
    nltk.download('stopwords')

class TextPreprocessor:
    def __init__(self, language='russian'):
        self.language = language
        self.stop_words = self._get_stop_words()
        
    def _get_stop_words(self):
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–æ–ø-—Å–ª–æ–≤ –∏–∑ NLTK"""
        try:
            from nltk.corpus import stopwords
            if self.language == 'russian':
                return set(stopwords.words('russian'))
            elif self.language == 'english':
                return set(stopwords.words('english'))
            else:
                return set()
        except:
            print("–ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ: –Ω–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å —Å—Ç–æ–ø-—Å–ª–æ–≤–∞ –∏–∑ NLTK, –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –±–∞–∑–æ–≤—ã–µ")
            if self.language == 'russian':
                return {'–∏', '–≤', '–≤–æ', '–Ω–µ', '—á—Ç–æ', '–æ–Ω', '–Ω–∞', '—è', '—Å', '—Å–æ', '–∫–∞–∫', '–∞', '—Ç–æ', '–≤—Å–µ', '–æ–Ω–∞',
                       '—Ç–∞–∫', '–µ–≥–æ', '–Ω–æ', '–¥–∞', '—Ç—ã', '–∫', '—É', '–∂–µ', '–≤—ã', '–∑–∞', '–±—ã', '–ø–æ', '—Ç–æ–ª—å–∫–æ', '–µ–µ', '–º–Ω–µ'}
            elif self.language == 'english':
                return {'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by'}
            else:
                return set()
    
    def clean_text(self, text):
        """–û—á–∏—Å—Ç–∫–∞ –∏ –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞"""
        if not isinstance(text, str) or pd.isna(text):
            return ""
        
        # –ü—Ä–∏–≤–µ–¥–µ–Ω–∏–µ –∫ –Ω–∏–∂–Ω–µ–º—É —Ä–µ–≥–∏—Å—Ç—Ä—É
        text = text.lower()
        
        # –£–¥–∞–ª–µ–Ω–∏–µ —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã—Ö —Å–∏–º–≤–æ–ª–æ–≤, –æ—Å—Ç–∞–≤–ª—è–µ–º –±—É–∫–≤—ã –∏ –æ—Å–Ω–æ–≤–Ω—ã–µ –∑–Ω–∞–∫–∏ –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è
        text = re.sub(r'[^a-zA-Z–∞-—è–ê-–Ø0-9\s.,!?]', '', text)
        
        # –£–¥–∞–ª–µ–Ω–∏–µ —Ü–∏—Ñ—Ä
        text = re.sub(r'\d+', '', text)
        
        # –£–¥–∞–ª–µ–Ω–∏–µ –ª–∏—à–Ω–∏—Ö –ø—Ä–æ–±–µ–ª–æ–≤
        text = re.sub(r'\s+', ' ', text).strip()
        
        # –£–¥–∞–ª–µ–Ω–∏–µ —Å—Ç–æ–ø-—Å–ª–æ–≤ –∏ –∫–æ—Ä–æ—Ç–∫–∏—Ö —Å–ª–æ–≤
        words = text.split()
        words = [word for word in words if word not in self.stop_words and len(word) > 2]
        
        return ' '.join(words)
    
    def preprocess_dataframe(self, df, text_column, label_column):
        """–ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –≤—Å–µ–≥–æ –¥–∞—Ç–∞—Ñ—Ä–µ–π–º–∞"""
        df_clean = df.copy()
        print("–ù–∞—á–∞—Ç–∞ –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–æ–≤...")
        
        # –ü—Ä–∏–º–µ–Ω—è–µ–º –æ—á–∏—Å—Ç–∫—É –∫ –∫–∞–∂–¥–æ–º—É —Ç–µ–∫—Å—Ç—É
        df_clean['cleaned_text'] = df_clean[text_column].apply(self.clean_text)
        
        # –£–¥–∞–ª—è–µ–º –ø—É—Å—Ç—ã–µ —Ç–µ–∫—Å—Ç—ã –ø–æ—Å–ª–µ –æ—á–∏—Å—Ç–∫–∏
        initial_len = len(df_clean)
        df_clean = df_clean[df_clean['cleaned_text'].str.len() > 0]
        final_len = len(df_clean)
        
        print(f"–ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞. –£–¥–∞–ª–µ–Ω–æ {initial_len - final_len} –ø—É—Å—Ç—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤.")
        print(f"–û—Å—Ç–∞–ª–æ—Å—å {final_len} —Ç–µ–∫—Å—Ç–æ–≤ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞.")
        
        return df_clean

class ClassicalTextClassifier:
    def __init__(self, language='russian'):
        self.language = language
        self.models = {}
        self.vectorizers = {}
        self.metrics = {}
        self.preprocessor = TextPreprocessor(language)
        self.error_analysis = {}
        
    def load_and_prepare_data(self, file_path, text_column='cleaned_comment', label_column='label'):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –∏ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö"""
        print("–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö...")
        df = pd.read_csv(file_path)
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–∞–Ω–Ω—ã–µ
        print(f"–†–∞–∑–º–µ—Ä –∏—Å—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö: {df.shape}")
        print(f"–ö–æ–ª–æ–Ω–∫–∏: {df.columns.tolist()}")
        print(f"–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤:\n{df[label_column].value_counts().sort_index()}")
        
        # –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞
        df_processed = self.preprocessor.preprocess_dataframe(df, text_column, label_column)
        
        # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ –ø—Ä–∏–∑–Ω–∞–∫–∏ –∏ –º–µ—Ç–∫–∏
        X = df_processed['cleaned_text']
        y = df_processed[label_column]
        
        # –°—Ç—Ä–∞—Ç–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.2, random_state=42, stratify=y
        )
        
        print(f"\n–î–∞–Ω–Ω—ã–µ –ø–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω—ã:")
        print(f"–¢—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω–∞—è –≤—ã–±–æ—Ä–∫–∞: {len(X_train)} samples")
        print(f"–¢–µ—Å—Ç–æ–≤–∞—è –≤—ã–±–æ—Ä–∫–∞: {len(X_test)} samples")
        print(f"–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤ –≤ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω–æ–π –≤—ã–±–æ—Ä–∫–µ:\n{pd.Series(y_train).value_counts().sort_index()}")
        
        return X_train, X_test, y_train, y_test
    
    def prepare_features(self, X_train, X_test):
        """–í–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–æ–≤ —Å –ø–æ–º–æ—â—å—é TF-IDF"""
        vectorizer = TfidfVectorizer(
            max_features=10000, 
            ngram_range=(1, 3),  # –î–æ–±–∞–≤–ª—è–µ–º —Ç—Ä–∏–≥—Ä–∞–º–º—ã –¥–ª—è –ª—É—á—à–µ–≥–æ –∑–∞—Ö–≤–∞—Ç–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
            stop_words=list(self.preprocessor.stop_words),
            min_df=2,
            max_df=0.9,
            analyzer='word'
        )
            
        X_train_vec = vectorizer.fit_transform(X_train)
        X_test_vec = vectorizer.transform(X_test)
        
        self.vectorizers['tfidf'] = vectorizer
        print(f"–í–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è TF-IDF –∑–∞–≤–µ—Ä—à–µ–Ω–∞. –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å: {X_train_vec.shape}")
        return X_train_vec, X_test_vec
    
    def calculate_class_weights(self, y_train):
        """–†–∞—Å—á–µ—Ç –≤–µ—Å–æ–≤ –∫–ª–∞—Å—Å–æ–≤ –¥–ª—è –±–æ—Ä—å–±—ã —Å –¥–∏—Å–±–∞–ª–∞–Ω—Å–æ–º"""
        classes = np.unique(y_train)
        weights = compute_class_weight('balanced', classes=classes, y=y_train)
        class_weights = dict(zip(classes, weights))
        
        print(f"–í–µ—Å–∞ –∫–ª–∞—Å—Å–æ–≤ –¥–ª—è –±–æ—Ä—å–±—ã —Å –¥–∏—Å–±–∞–ª–∞–Ω—Å–æ–º: {class_weights}")
        return class_weights
    
    def train_models(self, X_train, y_train, X_test, y_test):
        """–û–±—É—á–µ–Ω–∏–µ –∫–ª–∞—Å—Å–∏—á–µ—Å–∫–∏—Ö –º–æ–¥–µ–ª–µ–π"""
        # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Ñ–∏—á
        X_train_tfidf, X_test_tfidf = self.prepare_features(X_train, X_test)
        
        # –†–∞—Å—á–µ—Ç –≤–µ—Å–æ–≤ –∫–ª–∞—Å—Å–æ–≤
        class_weights = self.calculate_class_weights(y_train)
        
        # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –º–æ–¥–µ–ª–µ–π —Å –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
        models_config = {
            'Logistic Regression': LogisticRegression(
                random_state=42, 
                class_weight=class_weights, 
                max_iter=2000,
                C=0.8,  # –ë–æ–ª–µ–µ —Å–∏–ª—å–Ω–∞—è —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è
                solver='liblinear'
            ),
            'Naive Bayes': MultinomialNB(
                alpha=0.5,  # –°–≥–ª–∞–∂–∏–≤–∞–Ω–∏–µ –¥–ª—è —Ä–µ–¥–∫–∏—Ö —Å–ª–æ–≤
                fit_prior=True
            ),
            'Random Forest': RandomForestClassifier(
                n_estimators=150, 
                random_state=42, 
                class_weight=class_weights,
                max_depth=15,
                min_samples_split=5,
                min_samples_leaf=2,
                max_features='sqrt'
            )
        }
        
        # –û–±—É—á–µ–Ω–∏–µ –∏ –æ—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–µ–π
        for name, model in models_config.items():
            print(f"\nüîß –û–±—É—á–µ–Ω–∏–µ {name}...")
            model.fit(X_train_tfidf, y_train)
            
            # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
            y_pred = model.predict(X_test_tfidf)
            y_proba = model.predict_proba(X_test_tfidf)
            
            # –†–∞—Å—á–µ—Ç –º–µ—Ç—Ä–∏–∫
            self._calculate_metrics(name, y_test, y_pred, y_proba, X_test)
            self.models[name] = model
            
        print("\n‚úÖ –û–±—É—á–µ–Ω–∏–µ –≤—Å–µ—Ö –º–æ–¥–µ–ª–µ–π –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")
    
    def _calculate_metrics(self, model_name, y_true, y_pred, y_proba, X_test):
        """–†–∞—Å—á–µ—Ç –º–µ—Ç—Ä–∏–∫ –¥–ª—è –º–æ–¥–µ–ª–∏"""
        metrics = {
            'accuracy': accuracy_score(y_true, y_pred),
            'macro_f1': f1_score(y_true, y_pred, average='macro'),
            'weighted_f1': f1_score(y_true, y_pred, average='weighted'),
            'confusion_matrix': confusion_matrix(y_true, y_pred),
            'classification_report': classification_report(y_true, y_pred, output_dict=True)
        }
        
        # ROC-AUC –∏ PR-AUC
        try:
            metrics['roc_auc_ovr'] = roc_auc_score(y_true, y_proba, multi_class='ovr', average='macro')
            metrics['roc_auc_ovo'] = roc_auc_score(y_true, y_proba, multi_class='ovo', average='macro')
            metrics['pr_auc'] = average_precision_score(y_true, y_proba, average='macro')
        except Exception as e:
            print(f"–ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ: –Ω–µ —É–¥–∞–ª–æ—Å—å —Ä–∞—Å—Å—á–∏—Ç–∞—Ç—å AUC –¥–ª—è {model_name}: {e}")
            metrics['roc_auc_ovr'] = 0
            metrics['roc_auc_ovo'] = 0
            metrics['pr_auc'] = 0
            
        self.metrics[model_name] = metrics
        
        # –ê–Ω–∞–ª–∏–∑ –æ—à–∏–±–æ–∫
        self._analyze_errors(model_name, y_true, y_pred, X_test)
        
        # –í—ã–≤–æ–¥ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        print(f"\nüìä –†–ï–ó–£–õ–¨–¢–ê–¢–´ {model_name.upper()}:")
        print(f"Accuracy: {metrics['accuracy']:.4f}")
        print(f"Macro F1: {metrics['macro_f1']:.4f}")
        print(f"Weighted F1: {metrics['weighted_f1']:.4f}")
        print(f"ROC-AUC (OvR): {metrics['roc_auc_ovr']:.4f}")
        print(f"PR-AUC: {metrics['pr_auc']:.4f}")
        
        # –î–µ—Ç–∞–ª—å–Ω—ã–π –æ—Ç—á–µ—Ç –ø–æ –∫–ª–∞—Å—Å–∞–º
        print(f"\n–î–µ—Ç–∞–ª—å–Ω—ã–π –æ—Ç—á–µ—Ç –ø–æ –∫–ª–∞—Å—Å–∞–º:")
        print(classification_report(y_true, y_pred, target_names=['–ù–µ–π—Ç—Ä–∞–ª—å–Ω—ã–π', '–û—Å–∫–æ—Ä–±–∏—Ç–µ–ª—å–Ω—ã–π', '–¢–æ–∫—Å–∏—á–Ω—ã–π']))
        
        # –í—ã–≤–æ–¥ confusion matrix
        self._print_confusion_matrix(model_name, metrics['confusion_matrix'])
    
    def _print_confusion_matrix(self, model_name, cm):
        """–ö—Ä–∞—Å–∏–≤—ã–π –≤—ã–≤–æ–¥ confusion matrix"""
        print(f"\nüìã CONFUSION MATRIX - {model_name.upper()}:")
        print(" " * 15 + "Predicted ‚Üí")
        print(" " * 12 + "–ù–µ–π—Ç—Ä  –û—Å–∫–æ—Ä–±  –¢–æ–∫—Å–∏—á")
        
        class_names = ['–ù–µ–π—Ç—Ä–∞–ª—å–Ω—ã–π', '–û—Å–∫–æ—Ä–±–∏—Ç–µ–ª—å–Ω—ã–π', '–¢–æ–∫—Å–∏—á–Ω—ã–π']
        for i, true_class in enumerate(class_names):
            row_label = f"True {true_class[:6]}"
            print(f"{row_label:12}", end="")
            for j in range(3):
                print(f"{cm[i, j]:6}  ", end="")
            print()
        
        # –ê–Ω–∞–ª–∏–∑ –º–∞—Ç—Ä–∏—Ü—ã
        total = cm.sum()
        accuracy = np.trace(cm) / total
        print(f"\n–û–±—â–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å: {accuracy:.3f}")
        
        # –ê–Ω–∞–ª–∏–∑ –ø–æ –∫–ª–∞—Å—Å–∞–º
        for i, class_name in enumerate(class_names):
            precision = cm[i,i] / cm[:,i].sum() if cm[:,i].sum() > 0 else 0
            recall = cm[i,i] / cm[i,:].sum() if cm[i,:].sum() > 0 else 0
            print(f"{class_name}: Precision={precision:.3f}, Recall={recall:.3f}")
    
    def _analyze_errors(self, model_name, y_true, y_pred, X_test):
        """–î–µ—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –æ—à–∏–±–æ–∫ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏"""
        errors = {
            'false_positives': [],  # –ù–µ–π—Ç—Ä–∞–ª—å–Ω—ã–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω—ã –∫–∞–∫ –æ–ø–∞—Å–Ω—ã–µ
            'false_negatives': [],  # –û–ø–∞—Å–Ω—ã–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω—ã –∫–∞–∫ –Ω–µ–π—Ç—Ä–∞–ª—å–Ω—ã–µ
            'confusions': []        # –û—à–∏–±–∫–∏ –º–µ–∂–¥—É –æ—Å–∫–æ—Ä–±–∏—Ç–µ–ª—å–Ω—ã–º –∏ —Ç–æ–∫—Å–∏—á–Ω—ã–º
        }
        
        for i, (true, pred, text) in enumerate(zip(y_true, y_pred, X_test)):
            text_length = len(str(text).split())
            
            if true == 0 and pred != 0:  # False Positive
                errors['false_positives'].append({
                    'text': text,
                    'true_class': true,
                    'pred_class': pred,
                    'length': text_length
                })
            elif true != 0 and pred == 0:  # False Negative
                errors['false_negatives'].append({
                    'text': text,
                    'true_class': true,
                    'pred_class': pred,
                    'length': text_length
                })
            elif true != pred and true != 0 and pred != 0:  # Confusion
                errors['confusions'].append({
                    'text': text,
                    'true_class': true,
                    'pred_class': pred,
                    'length': text_length
                })
        
        self.error_analysis[model_name] = errors
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –æ—à–∏–±–æ–∫
        print(f"\nüîç –ê–ù–ê–õ–ò–ó –û–®–ò–ë–û–ö –î–õ–Ø {model_name}:")
        print(f"False Positives (–Ω–µ–π—Ç—Ä–∞–ª—å–Ω—ã–µ ‚Üí –æ–ø–∞—Å–Ω—ã–µ): {len(errors['false_positives'])}")
        print(f"False Negatives (–æ–ø–∞—Å–Ω—ã–µ ‚Üí –Ω–µ–π—Ç—Ä–∞–ª—å–Ω—ã–µ): {len(errors['false_negatives'])}")
        print(f"Confusions (–æ—à–∏–±–∫–∏ –º–µ–∂–¥—É –∫–ª–∞—Å—Å–∞–º–∏): {len(errors['confusions'])}")
        
        # –ê–Ω–∞–ª–∏–∑ –¥–ª–∏–Ω—ã —Ç–µ–∫—Å—Ç–æ–≤ –≤ –æ—à–∏–±–∫–∞—Ö
        if errors['false_positives']:
            fp_lengths = [err['length'] for err in errors['false_positives']]
            print(f"–°—Ä–µ–¥–Ω—è—è –¥–ª–∏–Ω–∞ False Positive —Ç–µ–∫—Å—Ç–æ–≤: {np.mean(fp_lengths):.1f} —Å–ª–æ–≤")
        
        if errors['false_negatives']:
            fn_lengths = [err['length'] for err in errors['false_negatives']]
            print(f"–°—Ä–µ–¥–Ω—è—è –¥–ª–∏–Ω–∞ False Negative —Ç–µ–∫—Å—Ç–æ–≤: {np.mean(fn_lengths):.1f} —Å–ª–æ–≤")
    
    def plot_confusion_matrices(self):
        """–í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è confusion matrices"""
        fig, axes = plt.subplots(1, 3, figsize=(18, 5))
        
        for idx, (model_name, metrics) in enumerate(self.metrics.items()):
            cm = metrics['confusion_matrix']
            sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[idx],
                       xticklabels=['–ù–µ–π—Ç—Ä', '–û—Å–∫–æ—Ä–±', '–¢–æ–∫—Å–∏—á'],
                       yticklabels=['–ù–µ–π—Ç—Ä', '–û—Å–∫–æ—Ä–±', '–¢–æ–∫—Å–∏—á'],
                       cbar_kws={'shrink': 0.8})
            axes[idx].set_title(f'{model_name}\nAccuracy: {metrics["accuracy"]:.3f}', fontsize=12)
            axes[idx].set_xlabel('–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–π –∫–ª–∞—Å—Å', fontsize=10)
            axes[idx].set_ylabel('–ò—Å—Ç–∏–Ω–Ω—ã–π –∫–ª–∞—Å—Å', fontsize=10)
        
        plt.tight_layout()
        plt.show()
    
    def plot_metrics_comparison(self):
        """–°—Ä–∞–≤–Ω–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫ –≤—Å–µ—Ö –º–æ–¥–µ–ª–µ–π"""
        metrics_df = pd.DataFrame({
            model: {
                'Accuracy': metrics['accuracy'],
                'Macro F1': metrics['macro_f1'],
                'Weighted F1': metrics['weighted_f1'],
                'ROC-AUC': metrics['roc_auc_ovr'],
                'PR-AUC': metrics['pr_auc']
            }
            for model, metrics in self.metrics.items()
        }).T
        
        fig, axes = plt.subplots(2, 3, figsize=(18, 10))
        axes = axes.ravel()
        
        metrics_to_plot = ['Accuracy', 'Macro F1', 'Weighted F1', 'ROC-AUC', 'PR-AUC']
        
        for idx, metric in enumerate(metrics_to_plot):
            ax = axes[idx]
            colors = ['skyblue', 'lightgreen', 'coral']
            metrics_df[metric].plot(kind='bar', ax=ax, color=colors)
            ax.set_title(f'–°—Ä–∞–≤–Ω–µ–Ω–∏–µ {metric}', fontsize=14)
            ax.set_ylabel(metric, fontsize=12)
            ax.tick_params(axis='x', rotation=45)
            
            # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –∑–Ω–∞—á–µ–Ω–∏–π –Ω–∞ —Å—Ç–æ–ª–±—Ü—ã
            for p in ax.patches:
                ax.annotate(f'{p.get_height():.3f}', 
                           (p.get_x() + p.get_width() / 2., p.get_height()),
                           ha='center', va='bottom', fontsize=10)
        
        # –£–±–∏—Ä–∞–µ–º –ª–∏—à–Ω–∏–π subplot
        axes[5].set_visible(False)
        
        plt.tight_layout()
        plt.show()
        
        return metrics_df
    
    def detailed_error_analysis(self, X_test, y_test):
        """–î–µ—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –æ—à–∏–±–æ–∫ –ø–æ –≤—Å–µ–º –º–æ–¥–µ–ª—è–º"""
        print("\n" + "="*80)
        print("–î–ï–¢–ê–õ–¨–ù–´–ô –ê–ù–ê–õ–ò–ó –û–®–ò–ë–û–ö –ö–õ–ê–°–°–ò–§–ò–ö–ê–¶–ò–ò")
        print("="*80)
        
        # –ê–Ω–∞–ª–∏–∑ –¥–ª–∏–Ω—ã —Ç–µ–∫—Å—Ç–æ–≤
        text_lengths = [len(str(text).split()) for text in X_test]
        
        plt.figure(figsize=(15, 10))
        
        # 1. –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –¥–ª–∏–Ω—ã —Ç–µ–∫—Å—Ç–æ–≤
        plt.subplot(2, 3, 1)
        plt.hist(text_lengths, bins=50, alpha=0.7, color='skyblue', edgecolor='black')
        plt.xlabel('–î–ª–∏–Ω–∞ —Ç–µ–∫—Å—Ç–∞ (—Å–ª–æ–≤)')
        plt.ylabel('–ß–∞—Å—Ç–æ—Ç–∞')
        plt.title('–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –¥–ª–∏–Ω—ã —Ç–µ–∫—Å—Ç–æ–≤')
        plt.axvline(np.mean(text_lengths), color='red', linestyle='--', label=f'–°—Ä–µ–¥–Ω–µ–µ: {np.mean(text_lengths):.1f}')
        plt.legend()
        
        # 2. –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤
        plt.subplot(2, 3, 2)
        class_distribution = pd.Series(y_test).value_counts().sort_index()
        colors = ['green', 'orange', 'red']
        class_distribution.plot(kind='bar', color=colors)
        plt.xlabel('–ö–ª–∞—Å—Å')
        plt.ylabel('–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ')
        plt.title('–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤')
        plt.xticks([0, 1, 2], ['–ù–µ–π—Ç—Ä–∞–ª—å–Ω—ã–π', '–û—Å–∫–æ—Ä–±–∏—Ç–µ–ª—å–Ω—ã–π', '–¢–æ–∫—Å–∏—á–Ω—ã–π'], rotation=0)
        
        # 3. –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –æ—à–∏–±–æ–∫ –ø–æ –º–æ–¥–µ–ª—è–º
        plt.subplot(2, 3, 3)
        error_types = ['False Positives', 'False Negatives', 'Confusions']
        error_data = {}
        
        for model_name in self.metrics.keys():
            errors = self.error_analysis[model_name]
            error_data[model_name] = [
                len(errors['false_positives']),
                len(errors['false_negatives']), 
                len(errors['confusions'])
            ]
        
        x = np.arange(len(error_types))
        width = 0.25
        multiplier = 0
        
        for model_name, errors in error_data.items():
            offset = width * multiplier
            plt.bar(x + offset, errors, width, label=model_name)
            multiplier += 1
        
        plt.xlabel('–¢–∏–ø –æ—à–∏–±–∫–∏')
        plt.ylabel('–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ—à–∏–±–æ–∫')
        plt.title('–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –æ—à–∏–±–æ–∫ –ø–æ —Ç–∏–ø–∞–º')
        plt.xticks(x + width, error_types, rotation=45)
        plt.legend()
        
        # 4. –î–ª–∏–Ω–∞ —Ç–µ–∫—Å—Ç–æ–≤ –≤ –æ—à–∏–±–∫–∞—Ö
        plt.subplot(2, 3, 4)
        length_data = []
        labels = []
        
        for model_name, errors in self.error_analysis.items():
            if errors['false_positives']:
                fp_lengths = [err['length'] for err in errors['false_positives']]
                length_data.append(np.mean(fp_lengths))
                labels.append(f'{model_name}\nFP')
            
            if errors['false_negatives']:
                fn_lengths = [err['length'] for err in errors['false_negatives']]
                length_data.append(np.mean(fn_lengths))
                labels.append(f'{model_name}\nFN')
        
        plt.bar(labels, length_data, color=['lightblue', 'lightcoral'] * 3)
        plt.ylabel('–°—Ä–µ–¥–Ω—è—è –¥–ª–∏–Ω–∞ (—Å–ª–æ–≤)')
        plt.title('–°—Ä–µ–¥–Ω—è—è –¥–ª–∏–Ω–∞ —Ç–µ–∫—Å—Ç–æ–≤ –≤ –æ—à–∏–±–∫–∞—Ö')
        plt.xticks(rotation=45)
        
        # 5. –°—Ä–∞–≤–Ω–µ–Ω–∏–µ F1-score –ø–æ –∫–ª–∞—Å—Å–∞–º
        plt.subplot(2, 3, 5)
        class_f1_scores = {}
        
        for model_name, metrics in self.metrics.items():
            report = metrics['classification_report']
            class_f1_scores[model_name] = [
                report['0']['f1-score'],
                report['1']['f1-score'], 
                report['2']['f1-score']
            ]
        
        x = np.arange(3)
        width = 0.25
        multiplier = 0
        
        for model_name, f1_scores in class_f1_scores.items():
            offset = width * multiplier
            plt.bar(x + offset, f1_scores, width, label=model_name)
            multiplier += 1
        
        plt.xlabel('–ö–ª–∞—Å—Å')
        plt.ylabel('F1-Score')
        plt.title('F1-Score –ø–æ –∫–ª–∞—Å—Å–∞–º')
        plt.xticks(x + width, ['–ù–µ–π—Ç—Ä–∞–ª—å–Ω—ã–π', '–û—Å–∫–æ—Ä–±–∏—Ç–µ–ª—å–Ω—ã–π', '–¢–æ–∫—Å–∏—á–Ω—ã–π'], rotation=45)
        plt.legend()
        
        plt.tight_layout()
        plt.show()
        
        # –í—ã–≤–æ–¥ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
        print(f"\nüìà –°–¢–ê–¢–ò–°–¢–ò–ö–ê –î–ê–ù–ù–´–•:")
        print(f"–û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–µ—Å—Ç–æ–≤—ã—ÖÊ†∑Êú¨: {len(y_test)}")
        print(f"–°—Ä–µ–¥–Ω—è—è –¥–ª–∏–Ω–∞ —Ç–µ–∫—Å—Ç–∞: {np.mean(text_lengths):.1f} ¬± {np.std(text_lengths):.1f} —Å–ª–æ–≤")
        print(f"–ú–µ–¥–∏–∞–Ω–Ω–∞—è –¥–ª–∏–Ω–∞: {np.median(text_lengths):.1f} —Å–ª–æ–≤")
        
        print(f"\nüìä –†–ê–°–ü–†–ï–î–ï–õ–ï–ù–ò–ï –ö–õ–ê–°–°–û–í:")
        for class_idx, count in class_distribution.items():
            class_name = ['–ù–µ–π—Ç—Ä–∞–ª—å–Ω—ã–π', '–û—Å–∫–æ—Ä–±–∏—Ç–µ–ª—å–Ω—ã–π', '–¢–æ–∫—Å–∏—á–Ω—ã–π'][class_idx]
            percentage = count / len(y_test) * 100
            print(f"  {class_name}: {count} samples ({percentage:.1f}%)")
        
        # –ê–Ω–∞–ª–∏–∑ –ø—Ä–∏—á–∏–Ω –æ—à–∏–±–æ–∫
        print(f"\nüîç –ü–†–ò–ß–ò–ù–´ –û–®–ò–ë–û–ö –ö–õ–ê–°–°–ò–§–ò–ö–ê–¶–ò–ò:")
        print("1. –î–ò–°–ë–ê–õ–ê–ù–° –ö–õ–ê–°–°–û–í: –ù–µ–π—Ç—Ä–∞–ª—å–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –±–æ–ª—å—à–µ, —á–µ–º –æ–ø–∞—Å–Ω—ã—Ö")
        print("2. –î–õ–ò–ù–ê –¢–ï–ö–°–¢–ê: –ö–æ—Ä–æ—Ç–∫–∏–µ —Ç–µ–∫—Å—Ç—ã —Å–ª–æ–∂–Ω–µ–µ –∫–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞—Ç—å")
        print("3. –ö–û–ù–¢–ï–ö–°–¢: –û–¥–∏–Ω–∞–∫–æ–≤—ã–µ —Å–ª–æ–≤–∞ –≤ —Ä–∞–∑–Ω–æ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ –º–æ–≥—É—Ç –∏–º–µ—Ç—å —Ä–∞–∑–Ω—É—é —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å")
        print("4. –°–ê–†–ö–ê–ó–ú –ò –ò–†–û–ù–ò–Ø: –°–ª–æ–∂–Ω–æ –¥–µ—Ç–µ–∫—Ç–∏—Ä–æ–≤–∞—Ç—å –±–µ–∑ –≥–ª—É–±–æ–∫–æ–≥–æ –ø–æ–Ω–∏–º–∞–Ω–∏—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞")
        print("5. –ù–û–í–ê–Ø –õ–ï–ö–°–ò–ö–ê: –ú–æ–¥–µ–ª—å –º–æ–∂–µ—Ç –Ω–µ –∑–Ω–∞—Ç—å –Ω–æ–≤—ã—Ö –æ—Å–∫–æ—Ä–±–∏—Ç–µ–ª—å–Ω—ã—Ö –≤—ã—Ä–∞–∂–µ–Ω–∏–π")

  

def run_classical_models(file_path, text_column='cleaned_comment', label_column='label', language='russian'):
    """
    –ü–æ–ª–Ω—ã–π –∑–∞–ø—É—Å–∫ –∫–ª–∞—Å—Å–∏—á–µ—Å–∫–∏—Ö –º–æ–¥–µ–ª–µ–π –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–æ–≤
    """
    print("üéØ –ó–ê–ü–£–°–ö –ö–õ–ê–°–°–ò–ß–ï–°–ö–ò–• –ú–û–î–ï–õ–ï–ô –ú–ê–®–ò–ù–ù–û–ì–û –û–ë–£–ß–ï–ù–ò–Ø")
    print("="*70)
    print(f"–Ø–∑—ã–∫: {language}")
    print(f"–§–∞–π–ª –¥–∞–Ω–Ω—ã—Ö: {file_path}")
    print("="*70)
    
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞
    classifier = ClassicalTextClassifier(language=language)
    
    # –ó–∞–≥—Ä—É–∑–∫–∞ –∏ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
    X_train, X_test, y_train, y_test = classifier.load_and_prepare_data(
        file_path, text_column, label_column
    )
    
    # –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π
    classifier.train_models(X_train, y_train, X_test, y_test)
    
    # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    classifier.plot_confusion_matrices()
    metrics_df = classifier.plot_metrics_comparison()
    
    # –î–µ—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –æ—à–∏–±–æ–∫
    classifier.detailed_error_analysis(X_test, y_test)
    
    # –í—ã–≤–æ–¥ –∏—Ç–æ–≥–æ–≤–æ–π —Ç–∞–±–ª–∏—Ü—ã
    print("\n" + "="*80)
    print("–ò–¢–û–ì–û–í–ê–Ø –¢–ê–ë–õ–ò–¶–ê –†–ï–ó–£–õ–¨–¢–ê–¢–û–í")
    print("="*80)
    print(metrics_df.round(4))
    
    # –ê–Ω–∞–ª–∏–∑ –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏
    best_model = metrics_df['Macro F1'].idxmax()
    best_score = metrics_df.loc[best_model, 'Macro F1']
    print(f"\nüèÜ –õ–£–ß–®–ê–Ø –ú–û–î–ï–õ–¨: {best_model} (Macro F1: {best_score:.4f})")
    
    
    return classifier

if __name__ == "__main__":
    # –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
    try:
        results = run_classical_models(
            file_path='train_m.csv',  # –ó–∞–º–µ–Ω–∏—Ç–µ –Ω–∞ –ø—É—Ç—å –∫ –≤–∞—à–µ–º—É —Ñ–∞–π–ª—É
            text_column='comment_text', 
            label_column='label',
            language='english' 
        )
      
