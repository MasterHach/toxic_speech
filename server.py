from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from datetime import datetime
import os
import json
import logging
from typing import Optional, Dict, Any
import numpy as np
from tensorflow.keras.models import load_model
from tensorflow.keras.preprocessing.sequence import pad_sequences
import pickle
import json

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('server.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

app = FastAPI(title="Audio Text Analyzer Server")

class TextData(BaseModel):
    text: str
    user_id: int
    username: Optional[str] = None
    chat_id: Optional[int] = None
    timestamp: Optional[str] = None

class AnalysisResult(BaseModel):
    toxicity_level: str
    confidence: float
    category: str
    flags: list[str]

# –ü–∞–ø–∫–∏ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
OUTPUT_DIR = "saved_texts"
LOG_DIR = "logs"
os.makedirs(OUTPUT_DIR, exist_ok=True)
os.makedirs(LOG_DIR, exist_ok=True)

class TextAnalyzer:
    """–ö–ª–∞—Å—Å –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ —Ç–µ–∫—Å—Ç–∞ —Å –ø–æ–º–æ—â—å—é AI –º–æ–¥–µ–ª–∏"""
    
    def __init__(self):
        self.model = None
        self.load_model()
    
    def load_model(self):
        """–ó–∞–≥—Ä—É–∑–∫–∞ AI –º–æ–¥–µ–ª–∏"""
        try:
            # –ó–¥–µ—Å—å –¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å –∑–∞–≥—Ä—É–∑–∫–∞ —Ç–≤–æ–µ–π –º–æ–¥–µ–ª–∏
            self.model = load_model('toxicity_classifier_model (1).h5')
            with open('toxicity_classifier_tokenizer (1).pkl', 'rb') as f:
                self.tokenizer = pickle.load(f)
            with open('toxicity_classifier_config (1).json', 'r') as f:
                self.config = json.load(f)
            # –≠—Ç–æ –ø—Ä–∏–º–µ—Ä - –∑–∞–º–µ–Ω–∏ –Ω–∞ —Å–≤–æ—é —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—é
            logger.info("üîÑ –ó–∞–≥—Ä—É–∂–∞—é AI –º–æ–¥–µ–ª—å...")
            
            # –ü—Ä–∏–º–µ—Ä –∑–∞–≥–ª—É—à–∫–∏ - –∑–∞–º–µ–Ω–∏ –Ω–∞ —Å–≤–æ—é –º–æ–¥–µ–ª—å
            self.model_loaded = True
            logger.info("‚úÖ AI –º–æ–¥–µ–ª—å –≥–æ—Ç–æ–≤–∞ –∫ —Ä–∞–±–æ—Ç–µ")
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏: {e}")
            self.model_loaded = False
    
    def analyze_text(self, text: str) -> AnalysisResult:
        """–ê–Ω–∞–ª–∏–∑ —Ç–µ–∫—Å—Ç–∞ –Ω–∞ —Ç–æ–∫—Å–∏—á–Ω–æ—Å—Ç—å"""
        try:
            # if not self.model_loaded:
            #     return self._fallback_analysis(text)
            
            # –ó–î–ï–°–¨ –í–°–¢–ê–í–¨ –ö–û–î –¢–í–û–ï–ô –ú–û–î–ï–õ–ò
            # –ü—Ä–∏–º–µ—Ä —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏:
            #toxicity_score = self._predict_toxicity(text)

            sequence = self.tokenizer.texts_to_sequences([text])
            padded_sequence = pad_sequences(sequence, maxlen=self.config['max_len'], padding='post', truncating='post')
            
            # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
            prediction = self.model.predict(padded_sequence, verbose=0)
            
            # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –Ω–æ–º–µ—Ä –∫–ª–∞—Å—Å–∞ —Å –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å—é
            result = np.argmax(prediction[0])
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —É—Ä–æ–≤–µ–Ω—å —Ç–æ–∫—Å–∏—á–Ω–æ—Å—Ç–∏
            if result == 2:
                category = "toxic"
            elif result == 1:
                category = "offensive"
            else:
                category = "neutral"
            
            return AnalysisResult(
                toxicity_level=str(result),
                confidence=0.0,
                category=category,
                flags=[]
            )
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ —Ç–µ–∫—Å—Ç–∞: {e}")
            return "None"
    
    # def _predict_toxicity(self, text: str) -> float:
    #     """–ó–∞–≥–ª—É—à–∫–∞ –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è —Ç–æ–∫—Å–∏—á–Ω–æ—Å—Ç–∏ - –∑–∞–º–µ–Ω–∏ –Ω–∞ —Å–≤–æ—é –º–æ–¥–µ–ª—å"""
    #     # –ü—Ä–∏–º–µ—Ä –ø—Ä–æ—Å—Ç–æ–π —ç–≤—Ä–∏—Å—Ç–∏–∫–∏
    #     toxic_words = ['–¥—É—Ä–∞–∫', '–∏–¥–∏–æ—Ç', '–º—É–¥–∞–∫', '—Å–≤–æ–ª–æ—á—å', '—É–±–ª—é–¥–æ–∫']
    #     text_lower = text.lower()
        
    #     for word in toxic_words:
    #         if word in text_lower:
    #             return 0.9
        
    #     if any(char in text for char in '!@#$%^&*') and len(text) < 20:
    #         return 0.7
            
    #     return 0.1
    
    # def _fallback_analysis(self, text: str) -> AnalysisResult:
    #     """–†–µ–∑–µ—Ä–≤–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –µ—Å–ª–∏ –º–æ–¥–µ–ª—å –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞"""
    #     return AnalysisResult(
    #         toxicity_level="unknown",
    #         confidence=0.0,
    #         category="not_analyzed", 
    #         flags=["model_not_available"]
    #     )

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞
analyzer = TextAnalyzer()

def save_to_json(data: dict, filename: str):
    """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –≤ JSON —Ñ–∞–π–ª"""
    try:
        filepath = os.path.join(OUTPUT_DIR, filename)
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        return filepath
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è JSON: {e}")
        return None

def log_user_activity(user_id: int, username: str, text: str, analysis: AnalysisResult):
    """–õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è"""
    log_entry = {
        "timestamp": datetime.now().isoformat(),
        "user_id": user_id,
        "username": username,
        "text_length": len(text),
        "toxicity_level": analysis.toxicity_level,
        "category": analysis.category,
        "confidence": analysis.confidence,
        "flags": analysis.flags
    }
    
    # –õ–æ–≥ –≤ —Ñ–∞–π–ª
    log_file = os.path.join(LOG_DIR, f"user_activity.log")
    with open(log_file, 'a', encoding='utf-8') as f:
        f.write(json.dumps(log_entry, ensure_ascii=False) + '\n')
    
    # –¢–∞–∫–∂–µ –≤ –∫–æ–Ω—Å–æ–ª—å
    logger.info(f"üë§ User {user_id} ({username}): {analysis.toxicity_level} label")

@app.post("/analyze_text")
async def analyze_text(data: TextData):
    """–û—Å–Ω–æ–≤–Ω–æ–π endpoint –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ —Ç–µ–∫—Å—Ç–∞"""
    try:
        start_time = datetime.now()
        
        # –î–æ–±–∞–≤–ª—è–µ–º timestamp –µ—Å–ª–∏ –Ω–µ —É–∫–∞–∑–∞–Ω
        if not data.timestamp:
            data.timestamp = datetime.now().isoformat()
        
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Ç–µ–∫—Å—Ç
        analysis_result = analyzer.analyze_text(data.text)
        
        # –ì–æ—Ç–æ–≤–∏–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
        save_data = {
            "user_info": {
                "user_id": data.user_id,
                "username": data.username,
                "chat_id": data.chat_id
            },
            "text_data": {
                "text": data.text,
                "timestamp": data.timestamp,
                "length": len(data.text)
            },
            "analysis_result": {
                "toxicity_level": analysis_result.toxicity_level,
                "confidence": analysis_result.confidence,
                "category": analysis_result.category,
                "flags": analysis_result.flags,
                "analysis_timestamp": datetime.now().isoformat()
            }
        }
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ —Ñ–∞–π–ª
        filename = f"user_{data.user_id}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        filepath = save_to_json(save_data, filename)
        
        # –õ–æ–≥–∏—Ä—É–µ–º –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç—å
        log_user_activity(
            data.user_id, 
            data.username or "unknown", 
            data.text, 
            analysis_result
        )
        
        processing_time = (datetime.now() - start_time).total_seconds()
        
        logger.info(f"‚úÖ –¢–µ–∫—Å—Ç –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω –∑–∞ {processing_time:.2f}—Å")
        
        return {
            "status": "success",
            "filename": filename,
            "analysis": analysis_result.dict(),
            "processing_time": processing_time,
            "message": "Text analyzed and saved successfully"
        }
        
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ —Ç–µ–∫—Å—Ç–∞: {e}")
        raise HTTPException(status_code=500, detail=f"Analysis error: {str(e)}")

@app.get("/stats")
async def get_stats():
    """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ —Å–µ—Ä–≤–µ—Ä–∞"""
    try:
        # –°—á–∏—Ç–∞–µ–º —Ñ–∞–π–ª—ã
        json_files = [f for f in os.listdir(OUTPUT_DIR) if f.endswith('.json')]
        
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –ª–æ–≥ –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏
        activity_log = os.path.join(LOG_DIR, "user_activity.log")
        user_activities = []
        
        if os.path.exists(activity_log):
            with open(activity_log, 'r', encoding='utf-8') as f:
                for line in f:
                    if line.strip():
                        user_activities.append(json.loads(line))
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ —Ç–æ–∫—Å–∏—á–Ω–æ—Å—Ç–∏
        toxicity_stats = {}
        for activity in user_activities:
            level = activity.get('toxicity_level', 'unknown')
            toxicity_stats[level] = toxicity_stats.get(level, 0) + 1
        
        return {
            "total_texts_analyzed": len(json_files),
            "toxicity_statistics": toxicity_stats,
            "unique_users": len(set(a.get('user_id') for a in user_activities)),
            "server_uptime": "active"  # –ú–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å —Ä–µ–∞–ª—å–Ω–æ–µ –≤—Ä–µ–º—è
        }
        
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏: {e}")
        return {"error": "Could not retrieve statistics"}

@app.get("/user_activity/{user_id}")
async def get_user_activity(user_id: int, limit: int = 10):
    """–ü–æ–ª—É—á–µ–Ω–∏–µ –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è"""
    try:
        activity_log = os.path.join(LOG_DIR, "user_activity.log")
        user_activities = []
        
        if os.path.exists(activity_log):
            with open(activity_log, 'r', encoding='utf-8') as f:
                for line in f:
                    if line.strip():
                        activity = json.loads(line)
                        if activity.get('user_id') == user_id:
                            user_activities.append(activity)
        
        return {
            "user_id": user_id,
            "total_activities": len(user_activities),
            "recent_activities": user_activities[-limit:]
        }
        
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è: {e}")
        return {"error": "Could not retrieve user activity"}

@app.get("/")
async def root():
    """–ö–æ—Ä–Ω–µ–≤–æ–π endpoint"""
    return {
        "message": "Audio Text Analyzer Server is running", 
        "endpoints": {
            "analyze_text": "POST /analyze_text",
            "stats": "GET /stats", 
            "user_activity": "GET /user_activity/{user_id}"
        }
    }

if __name__ == "__main__":
    import uvicorn
    logger.info("üöÄ –ó–∞–ø—É—Å–∫ —Å–µ—Ä–≤–µ—Ä–∞ –∞–Ω–∞–ª–∏–∑–∞ —Ç–µ–∫—Å—Ç–∞...")
    uvicorn.run(app, host="0.0.0.0", port=8000)
